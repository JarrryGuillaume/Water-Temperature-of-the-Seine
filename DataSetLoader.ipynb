{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DataSet import WaterTemperatureDataSet\n",
    "\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"DataSets\" \n",
    "WTA1_Dataset = pd.read_csv(folder + \"\\WTA1_DataSet.csv\")\n",
    "\n",
    "WTA1_Dataset.drop([\"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = WaterTemperatureDataSet(WTA1_Dataset, target_cols=[\"Water_Avg\"], numerical_cols=['Air_Min', 'Air_Max', \"Air_Avg\"], \n",
    "                                categorical_cols=[\"\"], discriminator_col=\"TimeSeries\")\n",
    "\n",
    "train_iter, test_iter = Data.get_loaders(batch_size=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = train_iter._get_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([365, 3]) torch.Size([365, 1])\n"
     ]
    }
   ],
   "source": [
    "tensor = iterator.next()\n",
    "print(tensor[0].size(), tensor[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTMs import WaterLSTM\n",
    "from Loss.dilate import dilate_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def trainLSTM(neuralNetwork, train_iter, learning_rate, epochs, show_training=True, print_every=5): \n",
    "    device = get_default_device()\n",
    "\n",
    "    neuralNetwork.train()\n",
    "    neuralNetwork.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(neuralNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "    total_loss, shape_loss, temporal_loss = [], [], []\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for series, target in train_iter: \n",
    "            series.to(device)\n",
    "            target.to(device)\n",
    "\n",
    "            predictions = neuralNetwork(series)\n",
    "\n",
    "            loss, loss_shape, loss_temporal = torch.tensor(0), torch.tensor(0), torch.tensor(0)\n",
    "            loss, loss_shape, loss_temporal = loss_function(predictions, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss.append(loss)\n",
    "        shape_loss.append(loss_shape)\n",
    "        temporal_loss.append(loss_temporal)\n",
    "\n",
    "        if show_training:\n",
    "            if epoch % print_every == 0: \n",
    "                print('epoch ', epoch, ' loss ', loss, ' loss shape ', loss_shape, ' loss temporal ', loss_temporal.item())\n",
    "\n",
    "\n",
    "    epochs = np.arange(0, epochs)\n",
    "    plt.plot(epochs, total_loss, c='blue')\n",
    "    plt.plot(epochs, shape_loss, c='green')\n",
    "    plt.plot(epochs, temporal_loss, c='red')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Losses from DILATE Loss\")\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(nn.Module): \n",
    "    def __init__(self, hidden_layers=178):\n",
    "        super(Simple, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.l1 = nn.Linear(3, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        self.l3 = nn.Linear(128, 365)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = torch.tanh(self.l1(x))\n",
    "        x = torch.sigmoid(self.l2(x))\n",
    "        output = self.l3(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([365, 1])) that is different to the input size (torch.Size([365, 365])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m Simple()\n\u001b[1;32m----> 3\u001b[0m trainLSTM(model, train_iter, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, print_every\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[46], line 26\u001b[0m, in \u001b[0;36mtrainLSTM\u001b[1;34m(neuralNetwork, train_iter, learning_rate, epochs, show_training, print_every)\u001b[0m\n\u001b[0;32m     23\u001b[0m predictions \u001b[39m=\u001b[39m neuralNetwork(series)\n\u001b[0;32m     25\u001b[0m loss, loss_shape, loss_temporal \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m loss, loss_shape, loss_temporal \u001b[39m=\u001b[39m loss_function(predictions, target)\n\u001b[0;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\guill\\miniconda3\\lib\\site-packages\\torch\\_tensor.py:723\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    714\u001b[0m     \u001b[39m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[0;32m    715\u001b[0m     \u001b[39m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    720\u001b[0m     \u001b[39m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     \u001b[39m# See gh-54457\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 723\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39miteration over a 0-d tensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    724\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[0;32m    725\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    726\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt change the number of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    727\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39miterations executed (and might lead to errors or silently give \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    728\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mincorrect results).\u001b[39m\u001b[39m'\u001b[39m, category\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mTracerWarning, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "model = Simple()\n",
    "\n",
    "trainLSTM(model, train_iter, learning_rate=0.1, epochs=1000, print_every=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d9c640c3ed48d17756f3e56bca34eb03a14e1f9680c61796106fb92fbd3927d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
